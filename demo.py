import sys
import arrow
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from base import BaseDeepAttentionHawkes
from models import SelfCorrectingAttentionTemporalHawkes, SelfExcitingAttentionTemporalHawkes, AttentionSpatialHawkes

import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

def simulation_hawkes_piecewise():
    """
    Simulation experiment using synthetic sequences generated by a Hawkes process  
    with an exponential kernel.

    The synthetic dataset contains 5,000 sequences in maximum length of 28 where 
    mu = 10, beta = 15. 
    """
    np.random.seed(seed=1)

    # load data
    data = np.load("data/sim_data_piecewise/sim_data.npy")
    data = np.expand_dims(data, -1)

    # configuration
    data_size   = data.shape[0]
    batch_size  = 40
    max_seq_len = data.shape[1]
    print(data.shape)

    # training
    with tf.Session() as sess:
        # define attentive hawkes
        ah = SelfExcitingAttentionHawkes(
            model_name="hawkes_piecewise", 
            max_seq_len=max_seq_len, 
            n_heads=2, key_size=5, score_layers=[5, 5], lam_layers=[5, 5])

        # train or restore the model
        # - restore:
        # saver = tf.train.Saver()
        # saver.restore(sess, "saved_models/hawkes_piecewise/model.ckpt")
        # - train:
        ah.train(sess, data, epoches=70, batch_size=batch_size, 
            lr=2e-3, decay_rate=0.99, decay_steps=100, test_seqs=data[:2])

        # save trained parameters
        ah.save_model(sess, "saved_models/hawkes_piecewise/model.ckpt")
        # evaluate lambda values given sequences
        tlim    = [0., 1.]
        n_tgrid = 100
        lams    = ah.evaluate_lam(sess, data[:5], tlim=tlim, n_tgrid=n_tgrid) # [batch_size, n_tgrid]

        # visualization
        for i in range(5):
            plt.plot(np.linspace(0., 1., 100), lams[i])
            plt.show()


def simulation_hawkes_expkernel():
    """
    Simulation experiment using synthetic sequences generated by a Hawkes process  
    with an exponential kernel.

    The synthetic dataset contains 5,000 sequences in maximum length of 32 where 
    mu = 10, beta = 7. 
    """
    np.random.seed(seed=1)

    # load data
    data = np.load("data/sim_data_beta30/sim_data.npy")
    data = np.expand_dims(data, -1)

    # configuration
    data_size   = data.shape[0]
    batch_size  = 40
    max_seq_len = data.shape[1]
    print(data.shape)

    # training
    with tf.Session() as sess:
        # define attentive hawkes
        ah = SelfExcitingAttentionTemporalHawkes(
            model_name="hawkes_hawkes_exp", 
            max_seq_len=max_seq_len, 
            n_heads=1, key_size=7, score_layers=[3, 7], lam_layers=[7])

        # train or restore the model
        # - restore:
        # saver = tf.train.Saver()
        # saver.restore(sess, "saved_models/hawkes_exp_beta/model.ckpt")
        # - train:
        ah.train(sess, data, 
            epoches=100, batch_size=batch_size, 
            lr=5e-3, decay_rate=0.7, decay_steps=100, test_seqs=data[:2])

        # save trained parameters
        ah.save_model(sess, "saved_models/hawkes_hawkes_exp/model.ckpt")
        # evaluate lambda values given sequences
        tlim    = [0., 1.]
        n_tgrid = 100
        lams    = ah.evaluate_lam(sess, data[:5], tlim=tlim, n_tgrid=n_tgrid) # [batch_size, n_tgrid]

        # visualization
        for i in range(5):
            plt.plot(np.linspace(0., 1., 100), lams[i])
            plt.show()

def simulation_selfcorrecting():
    """
    Simulation experiment using synthetic sequences generated by a self-correcting  
    process.

    The synthetic dataset contains 5,000 sequences in maximum length of 31 where 
    mu = 10, alpha = 0.25. 
    """
    np.random.seed(seed=1)

    # load data
    data = np.load("data/sim_data_selfcorrecting/sim_data.npy")
    data = np.expand_dims(data, -1)

    # configuration
    data_size   = data.shape[0]
    batch_size  = 40
    max_seq_len = data.shape[1]
    print(data.shape)

    # training
    with tf.Session() as sess:
        # define attentive hawkes
        ah = SelfCorrectingAttentionHawkes(
            model_name="hawkes_selfcorrecting", 
            max_seq_len=max_seq_len, 
            n_heads=3, key_size=5, score_layers=[5], lam_layers=[5])

        # train or restore the model
        # - restore:
        # saver = tf.train.Saver()
        # saver.restore(sess, "saved_models/hawkes_selfcorrecting/model.ckpt")
        # - train:
        ah.train(sess, data, epoches=50, batch_size=batch_size, 
            lr=1e-3, decay_rate=0.99, decay_steps=100, test_seqs=data[:2])

        # save trained parameters
        ah.save_model(sess, "saved_models/hawkes_selfcorrecting/model.ckpt")
        # evaluate lambda values given sequences
        tlim    = [0., 1.]
        n_tgrid = 100
        lams    = ah.evaluate_lam(sess, data[:5], tlim=tlim, n_tgrid=n_tgrid) # [batch_size, n_tgrid]

        # visualization
        for i in range(5):
            plt.plot(np.linspace(0., 1., 100), lams[i])
            print(data[i])
            plt.show()

def simulation_spatial_ETAS():
    """
    """
    np.random.seed(seed=1)

    # load data
    data = np.load("data/sim_data_ETAS/sim_data.npy")

    # configuration
    data_size   = data.shape[0]
    batch_size  = 40
    max_seq_len = data.shape[1]
    print(data.shape)

    # training
    with tf.Session() as sess:
        # define attentive hawkes
        ah = AttentionSpatialHawkes(
            model_name="hawkes_ETAS", 
            max_seq_len=max_seq_len, 
            n_heads=3, key_size=7, score_layers=[7], lam_layers=[7])

        # train or restore the model
        # - restore:
        # saver = tf.train.Saver()
        # saver.restore(sess, "saved_models/hawkes_exp_beta/model.ckpt")
        # - train:
        ah.train(sess, data, 
            epoches=100, batch_size=batch_size, 
            lr=1e-2, decay_rate=0.7, decay_steps=100, test_seqs=data[:2])

        # save trained parameters
        ah.save_model(sess, "saved_models/hawkes_ETAS/model.ckpt")
        # # evaluate lambda values given sequences
        # tlim    = [0., 1.]
        # slim    = [-1., 1.]
        # n_tgrid = 10
        # n_sgrid = 10
        # lams    = ah.evaluate_lam(sess, data[:5], tlim, slim, n_tgrid, n_sgrid) # [batch_size, n_tgrid]

        # # visualization
        # for i in range(5):
        #     plt.plot(np.linspace(0., 1., 100), lams[i])
        #     plt.show()

if __name__ == "__main__":
    simulation_spatial_ETAS()
    # simulation_hawkes_expkernel()